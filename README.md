
Spark & Hadoop Taxi Trips Analysis
---

## ðŸ“‘ Contents

1. Introduction
2. Requirements & Environment
3. File Structure
4. Execution Instructions
   * CSV â†’ Parquet Conversion
   * Query Q1: RDD / DataFrame
   * Query Q2: RDD / DataFrame
   * Query Q3: DataFrame & SQL (CSV vs Parquet)
   * Query Q4: SQL (CSV vs Parquet)
   * Query Q5: DataFrame (CSV vs Parquet)
   * Query Q6: DataFrame + Scaling
   * Part 1B: Optimizer Join Study
5. Observations & Performance Comparison

---

## Introduction

Analysis of real NYC TLC taxi trips data using **Apache Spark** on **HDFS**.  
Queries Q1â€“Q6 are implemented with RDD, DataFrame, SQL APIs and CSV vs Parquet formats, as well as a study of the join strategy of the Catalyst optimizer.

---

## Requirements & Environment

* Apache Hadoop â‰¥ 3.3
* Apache Spark â‰¥ 3.5
* Python 3.8+
* Kubernetes cluster & HDFS access
* Spark submit from Docker image `apache/spark`
* Settings in `spark-defaults.conf` (namespace, serviceAccount, etc.)

---

## File Structure

```
.
â”œâ”€â”€ csv_to_parquet.py            # Converts all CSV files to Parquet
â”œâ”€â”€ 1b.py                        # Part 1B: explain â€“ join optimizer study
â”‚
â”œâ”€â”€ q1_rdd.py                    # Q1 â€“ RDD API
â”œâ”€â”€ q1_df.py                     # Q1 â€“ DataFrame API (no UDF)
â”œâ”€â”€ q1_df_udf.py                 # Q1 â€“ DataFrame API (with UDF)
â”‚
â”œâ”€â”€ q2_rdd.py                    # Q2 â€“ RDD API
â”œâ”€â”€ q2_df.py                     # Q2 â€“ DataFrame API
â”œâ”€â”€ q2_sql.py                    # Q2 â€“ SparkSQL API
â”‚
â”œâ”€â”€ q3_df_csv.py                 # Q3 â€“ DataFrame API (CSV)
â”œâ”€â”€ q3_df_parquet.py             # Q3 â€“ DataFrame API (Parquet)
â”œâ”€â”€ q3_sql_csv.py                # Q3 â€“ SQL API (CSV)
â””â”€â”€ q3_sql_parquet.py            # Q3 â€“ SQL API (Parquet)

â”œâ”€â”€ q4_sql_csv.py                # Q4 â€“ SQL API (CSV)
â”œâ”€â”€ q4_sql_parquet.py            # Q4 â€“ SQL API (Parquet)
â”‚
â”œâ”€â”€ q5_df_csv.py                 # Q5 â€“ DataFrame API (CSV)
â””â”€â”€ q5_df_parquet.py             # Q5 â€“ DataFrame API (Parquet)

â””â”€â”€ q6_df.py                     # Q6 â€“ DataFrame API + scaling tests
â”‚
â”œâ”€â”€ report.pdf
```

---

## Execution Instructions

Replace `<username>` in the HDFS path with your own username.

### CSV â†’ Parquet Conversion

```bash
spark-submit \
  --master k8s://â€¦ \
  --deploy-mode cluster \
  hdfs://hdfs-namenode:9000/user/<username>/csv_to_parquet.py
```

---

### Query Q1

1. **RDD API**

   ```bash
   spark-submit â€¦ q1_rdd.py
   ```
2. **DataFrame API (no UDF)**

   ```bash
   spark-submit â€¦ q1_df.py
   ```
3. **DataFrame API (with UDF)**

   ```bash
   spark-submit â€¦ q1_df_udf.py
   ```

---

### Query Q2

* **RDD**:

  ```bash
  spark-submit â€¦ q2_rdd.py
  ```
* **DataFrame**:

  ```bash
  spark-submit â€¦ q2_df.py
  ```
* **SQL**:

  ```bash
  spark-submit â€¦ q2_sql.py
  ```

---

### Query Q3

* **DataFrame, CSV**:

  ```bash
  spark-submit â€¦ q3_df_csv.py
  ```
* **DataFrame, Parquet**:

  ```bash
  spark-submit â€¦ q3_df_parquet.py
  ```
* **SQL, CSV**:

  ```bash
  spark-submit â€¦ q3_sql_csv.py
  ```
* **SQL, Parquet**:

  ```bash
  spark-submit â€¦ q3_sql_parquet.py
  ```

---

### Query Q4

* **SQL, CSV**:

  ```bash
  spark-submit â€¦ q4_sql_csv.py
  ```
* **SQL, Parquet**:

  ```bash
  spark-submit â€¦ q4_sql_parquet.py
  ```

---

### Query Q5

* **DataFrame, CSV**:

  ```bash
  spark-submit â€¦ q5_df_csv.py
  ```
* **DataFrame, Parquet**:

  ```bash
  spark-submit â€¦ q5_df_parquet.py
  ```

---

### Query Q6

```bash
# Horizontal & vertical scaling experiments:
#   â€¢ 2 executors Ã— 4 cores / 8 GB
#   â€¢ 4 executors Ã— 2 cores / 4 GB
#   â€¢ 8 executors Ã— 1 core  / 2 GB
spark-submit \
  --conf spark.executor.instances=<n> \
  --conf spark.executor.cores=<c> \
  --conf spark.executor.memory=<m> \
  â€¦ q6_df.py
```

---

### Part 1B: Optimizer Join Study

```bash
spark-submit â€¦ 1b.py
```

In the script, `spark.sql("EXPLAIN â€¦")` is called for 50 records and then time and join type (Broadcast vs Shuffle) are measured.

---

## License & References

* Data: NYC TLC Trip Record Data
* Apache Spark & Hadoop documentation
* Parquet format: [https://parquet.apache.org/](https://parquet.apache.org/)
