Spark & Hadoop Taxi Trips Analysis
---

## ğŸ“‘ Î ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î±

1. Î•Î¹ÏƒÎ±Î³Ï‰Î³Î®
2. Î‘Ï€Î±Î¹Ï„Î®ÏƒÎµÎ¹Ï‚ & Î ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½
3. Î”Î¿Î¼Î® Î‘ÏÏ‡ÎµÎ¯Ï‰Î½
4. ÎŸÎ´Î·Î³Î¯ÎµÏ‚ Î•ÎºÏ„Î­Î»ÎµÏƒÎ·Ï‚

   * ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® CSV â†’ Parquet
   * Query Q1: RDD / DataFrame
   * Query Q2: RDD / DataFrame
   * Query Q3: DataFrame & SQL (CSV vs Parquet)
   * Query Q4: SQL (CSV vs Parquet)
   * Query Q5: DataFrame (CSV vs Parquet)
   * Query Q6: DataFrame + Scaling
   * Part 1B: ÎœÎµÎ»Î­Ï„Î· Optimizer Join
5. Î Î±ÏÎ±Ï„Î·ÏÎ®ÏƒÎµÎ¹Ï‚ & Î£ÏÎ³ÎºÏÎ¹ÏƒÎ· Î•Ï€Î¹Î´ÏŒÏƒÎµÏ‰Î½

---

## Î•Î¹ÏƒÎ±Î³Ï‰Î³Î®

Î‘Î½Î¬Î»Ï…ÏƒÎ· Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÏÎ½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ NYC TLC taxi trips Î¼Îµ **Apache Spark** Ï€Î¬Î½Ï‰ ÏƒÎµ **HDFS**.  
Î¥Î»Î¿Ï€Î¿Î¹Î¿ÏÎ½Ï„Î±Î¹ ÎµÏÏ‰Ï„Î®Î¼Î±Ï„Î± Q1â€“Q6 Î¼Îµ RDD, DataFrame, SQL APIs ÎºÎ±Î¹ CSV vs Parquet formats, ÎºÎ±Î¸ÏÏ‚ ÎºÎ±Î¹ Î¼ÎµÎ»Î­Ï„Î· Ï„Î·Ï‚ ÏƒÏ„ÏÎ±Ï„Î·Î³Î¹ÎºÎ®Ï‚ join Ï„Î¿Ï… Catalyst optimizer.

---

## Î‘Ï€Î±Î¹Ï„Î®ÏƒÎµÎ¹Ï‚ & Î ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½

* Apache Hadoop â‰¥ 3.3
* Apache Spark â‰¥ 3.5
* Python 3.8+
* Kubernetes cluster & HDFS Ï€ÏÏŒÏƒÎ²Î±ÏƒÎ·
* Spark submit Î±Ï€ÏŒ Docker image `apache/spark`
* Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ ÏƒÎµ `spark-defaults.conf` (namespace, serviceAccount Îº.Î»Ï€.)

---

## Î”Î¿Î¼Î® Î‘ÏÏ‡ÎµÎ¯Ï‰Î½

```
.
â”œâ”€â”€ csv_to_parquet.py            # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ CSV ÏƒÎµ Parquet
â”œâ”€â”€ 1b.py                        # Part 1Î’: explain â€“ Î¼ÎµÎ»Î­Ï„Î· join optimizer
â”‚
â”œâ”€â”€ q1_rdd.py                    # Q1 â€“ RDD API
â”œâ”€â”€ q1_df.py                     # Q1 â€“ DataFrame API (Ï‡Ï‰ÏÎ¯Ï‚ UDF)
â”œâ”€â”€ q1_df_udf.py                 # Q1 â€“ DataFrame API (Î¼Îµ UDF)
â”‚
â”œâ”€â”€ q2_rdd.py                    # Q2 â€“ RDD API
â”œâ”€â”€ q2_df.py                     # Q2 â€“ DataFrame API
â”œâ”€â”€ q2_sql.py                    # Q2 â€“ SparkSQL API
â”‚
â”œâ”€â”€ q3_df_csv.py                 # Q3 â€“ DataFrame API (CSV)
â”œâ”€â”€ q3_df_parquet.py             # Q3 â€“ DataFrame API (Parquet)
â”œâ”€â”€ q3_sql_csv.py                # Q3 â€“ SQL API (CSV)
â””â”€â”€ q3_sql_parquet.py            # Q3 â€“ SQL API (Parquet)

â”œâ”€â”€ q4_sql_csv.py                # Q4 â€“ SQL API (CSV)
â”œâ”€â”€ q4_sql_parquet.py            # Q4 â€“ SQL API (Parquet)
â”‚
â”œâ”€â”€ q5_df_csv.py                 # Q5 â€“ DataFrame API (CSV)
â””â”€â”€ q5_df_parquet.py             # Q5 â€“ DataFrame API (Parquet)

â””â”€â”€ q6_df.py                     # Q6 â€“ DataFrame API + scaling tests
â”‚
â”œâ”€â”€ report.pdf
```

---

## ÎŸÎ´Î·Î³Î¯ÎµÏ‚ Î•ÎºÏ„Î­Î»ÎµÏƒÎ·Ï‚

Î‘Î½Ï„Î¹ÎºÎ±Ï„Î±ÏƒÏ„Î®ÏƒÏ„Îµ `<username>` ÏƒÏ„Î¿Î½ HDFS path Î¼Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ±Ï‚ ÏŒÎ½Î¿Î¼Î± Ï‡ÏÎ®ÏƒÏ„Î·.

### ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® CSV â†’ Parquet

```bash
spark-submit \
  --master k8s://â€¦ \
  --deploy-mode cluster \
  hdfs://hdfs-namenode:9000/user/<username>/csv_to_parquet.py
```

---

### Query Q1

1. **RDD API**

   ```bash
   spark-submit â€¦ q1_rdd.py
   ```
2. **DataFrame API (Ï‡Ï‰ÏÎ¯Ï‚ UDF)**

   ```bash
   spark-submit â€¦ q1_df.py
   ```
3. **DataFrame API (Î¼Îµ UDF)**

   ```bash
   spark-submit â€¦ q1_df_udf.py
   ```

---

### Query Q2

* **RDD**:

  ```bash
  spark-submit â€¦ q2_rdd.py
  ```
* **DataFrame**:

  ```bash
  spark-submit â€¦ q2_df.py
  ```
* **SQL**:

  ```bash
  spark-submit â€¦ q2_sql.py
  ```

---

### Query Q3

* **DataFrame, CSV**:

  ```bash
  spark-submit â€¦ q3_df_csv.py
  ```
* **DataFrame, Parquet**:

  ```bash
  spark-submit â€¦ q3_df_parquet.py
  ```
* **SQL, CSV**:

  ```bash
  spark-submit â€¦ q3_sql_csv.py
  ```
* **SQL, Parquet**:

  ```bash
  spark-submit â€¦ q3_sql_parquet.py
  ```

---

### Query Q4

* **SQL, CSV**:

  ```bash
  spark-submit â€¦ q4_sql_csv.py
  ```
* **SQL, Parquet**:

  ```bash
  spark-submit â€¦ q4_sql_parquet.py
  ```

---

### Query Q5

* **DataFrame, CSV**:

  ```bash
  spark-submit â€¦ q5_df_csv.py
  ```
* **DataFrame, Parquet**:

  ```bash
  spark-submit â€¦ q5_df_parquet.py
  ```

---

### Query Q6

```bash
# Horizontal & vertical scaling experiments:
#   â€¢ 2 executors Ã— 4 cores / 8 GB
#   â€¢ 4 executors Ã— 2 cores / 4 GB
#   â€¢ 8 executors Ã— 1 core  / 2 GB
spark-submit \
  --conf spark.executor.instances=<n> \
  --conf spark.executor.cores=<c> \
  --conf spark.executor.memory=<m> \
  â€¦ q6_df.py
```

---

### Part 1B: ÎœÎµÎ»Î­Ï„Î· Optimizer Join

```bash
spark-submit â€¦ 1b.py
```

Î£Ï„Î¿ script ÎºÎ±Î»ÎµÎ¯Ï„Î±Î¹ `spark.sql("EXPLAIN â€¦")` Î³Î¹Î± 50 ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚ ÎºÎ±Î¹ Î¼ÎµÏ„Î¬ Î¼ÎµÏ„ÏÎ¬Ï„Î±Î¹ Ï‡ÏÏŒÎ½Î¿Ï‚ ÎºÎ±Î¹ ÎµÎ¯Î´Î¿Ï‚ join (Broadcast vs Shuffle).

---

## License & Î‘Î½Î±Ï†Î¿ÏÎ­Ï‚

* Î”ÎµÎ´Î¿Î¼Î­Î½Î±: NYC TLC Trip Record Data
* Apache Spark & Hadoop documentation
* Parquet format: [https://parquet.apache.org/](https://parquet.apache.org/)
